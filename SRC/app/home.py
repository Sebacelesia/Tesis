# app.py â€” Pipeline completo en 2 etapas por bloque:
# Etapa 1: Prompt 2 + Prompt 4
# Etapa 2: Prompt 5 + Prompt 3 + tool CV_TAG
import io
import os
import json
import shutil
import tempfile
import textwrap
import re
import random
from typing import Optional, List, Callable, Tuple

import requests
import streamlit as st

# ====== PARÃMETROS FIJOS ======
OLLAMA_ENDPOINT = "http://localhost:11434"
MODEL_NAME      = "qwen3:8b"
TEMPERATURE     = 0.0

USE_CHUNKING          = True            # si el texto supera MAX_CHARS_PER_CHUNK, se parte
MAX_CHARS_PER_CHUNK   = 15000           # caracteres por chunk de texto (del documento)
OVERLAP               = 0               # solapamiento entre chunks (en caracteres)

# Procesar de a N pÃ¡ginas de PDF por bloque lÃ³gico
PAGES_PER_BLOCK       = 2               # pÃ¡ginas por bloque

# Importante para evitar cortes por contexto/salida en Ollama:
NUM_CTX               = 16384           # contexto (tokens del modelo) en Ollama
NUM_PREDICT           = 9000            # tokens de salida mÃ¡ximos

# Flag de debug para ver longitudes en consola
DEBUG_PIPELINE        = True

# ====== PROMPT 1: extraer datos del encabezado ======
PROMPT1_TEMPLATE = (
    """Eres un especialista en extraer datos de documentos clinicos.

Todos los documentos estan organizados por secciones indicadas al comienzo de cada una de ellas como cabezal. Por ejemplo:

--- SecciÃ³n 2 ---

La tarea que necesito que hagas es la siguiente:

De la "secciÃ³n 0 (Encabezado)" del siguiente fragmento preciso que extraigas el nombre completo, el documento y la direcciÃ³n del paciente.
La respuesta que me des tiene que incluir los tres datos en formato de lista en lenguaje python. Es decir, por ejemplo: ["Juan Perez", "1234567-8", "Avenida Libertad 109"]
La respuesta que me des no debe incluir comentarios de ningun tipo, solamente la lista que te pedÃ­ previamente en ese formato.

El fragmento es el siguiente:
{text}"""
)

# ====== PROMPT 2: censurar datos usando la lista ======
PROMPT2_TEMPLATE = """
Eres un especialista en censurar datos de personas en documentos.

El objetivo general es que, a continuaciÃ³n, te darÃ© una lista con datos y debes buscar dichos datos en el texto y siempre que estos aparezcan reemplazarlos por '[CENSURADO]' siguiendo ciertos criterios.

Dentro de la lista encontraras nombres, apellidos, un documento y una direcciÃ³n de residencia.

Criterios:
1) Siempre que detectes un nombre o apellido de los que estÃ¡ en la lista cambialos por [CENSURADO].
2) Siempre que detectes el documento de la lista en el documento cambialo por [CENSURADO]. Este documento no tiene porque aparecer en el documento tal cual como estÃ¡ en la lista. Por dar algunos ejemplos, si en la lista estÃ¡ "12345678" y en el documento encuentras "1234567-8", "1.234.567-8" o "1234567 8" esto debes cambiar por [CENSURADO] tambiÃ©n.
3) Siempre que detectes la direcciÃ³n de la lista en el documento cambialo por [CENSURADO]. Esta direcciÃ³n de la lista no tiene porque ser idÃ©ntica a la que encuentres en el documento. Por dar algunos ejemplos, si en la lista estÃ¡ "Avenida Libertad 123" y en el documento aparece "Av. Libertad 123" esto debes cambiar por [CENSURADO] tambiÃ©n. Si en la lista estÃ¡ "Mateo CortÃ©z 2395" y en el documento estÃ¡ "M. CortÃ©z 2395", "CortÃ©z 2395" o incluso "CortÃ©z numero 2395", esto debes cambiar por [CENSURADO] tambiÃ©n.
4) Por favor manten el formato (no utilices negritas ni aumentes el tamaÃ±o de la letra).
5) MUY IMPORTANTE: si en el texto NO encuentras ninguno de los datos de la lista, devuelve el texto ORIGINAL sin ningÃºn cambio y sin aÃ±adir frases como "no hay nada que censurar" ni otros comentarios.
6) Devuelve ÃšNICAMENTE el documento censurado (o el original si no hay cambios), sin explicaciones ni notas adicionales.

A continuaciÃ³n te muestro la lista:

{lista}

A continuaciÃ³n te comparto el documento:
{text}
"""

# ====== PROMPT 3: marcar carga viral con [[CV_TAG: ...]] ======
PROMPT3_TEMPLATE = """
Eres un especialista en anÃ¡lisis de textos clÃ­nicos en espaÃ±ol.

Tu tarea es identificar TODAS las menciones de carga viral en el texto y marcarlas con una etiqueta especial.

Instrucciones obligatorias:
1) Considera como menciÃ³n de carga viral expresiones en las que aparezcan tÃ©rminos como
   "carga viral", "Carga viral", "CV", "cv" cerca de un nÃºmero, por ejemplo:
   - "carga viral: 120000"
   - "carga viral 120000"
   - "CV: 120000"
   - "cv: 120000"
   - "CV 120000"
   u otras variantes similares.
2) Reemplaza ÃšNICAMENTE el nÃºmero por una marca con este formato EXACTO:
   - "cv: [[CV_TAG: 120000]]"
   - "CV: [[CV_TAG: -34000]]"
   - "Carga viral: [[CV_TAG: 45000]]"
   Dentro de [[CV_TAG: ...]] debes colocar el valor numÃ©rico original tal como aparece en el texto, incluyendo el signo si fuera negativo.
3) No agregues comentarios, explicaciones ni notas adicionales. Devuelve exclusivamente el texto (con las marcas aplicadas si las hay).

Texto a procesar:
{text}
"""

# ====== PROMPT 4: anonimizaciÃ³n general final ======
PROMPT4_TEMPLATE = """
Eres un asistente especializado en anonimizar historias clÃ­nicas en espaÃ±ol.

INSTRUCCIONES OBLIGATORIAS
1) Sustituye SOLO los siguientes datos personales por estos placeholders exactos:
   - Nombres y apellidos de personas de cualquier origen y en cualquier parte del documento (pacientes, familiares, mÃ©dicos) â†’ [CENSURADO]
   - TelÃ©fonos (reemplaza secuencias de 7+ dÃ­gitos o con separadores como +598, -, espacios, parÃ©ntesis; sean nacionales o internacionales) â†’ [CENSURADO]
   - CÃ©dulas de identidad / documentos â†’ [CENSURADO]
   - Direcciones postales/domicilios (calle/avenida + nÃºmero, esquinas, apartamento, barrio) â†’ [CENSURADO]
2) Conserva TODO lo demÃ¡s sin cambios: sÃ­ntomas, diagnÃ³sticos, dosis, resultados, unidades, abreviaturas, signos de puntuaciÃ³n, mayÃºsculas/minÃºsculas.
3) NO anonimices ni modifiques valores de carga viral (no borres ni reemplaces nÃºmeros que sigan a 'CV', 'cv' o 'carga viral').
4) Devuelve ÃšNICAMENTE el texto anonimizado (o el original si no hay cambios), sin explicaciones ni encabezados adicionales.

Texto a anonimizar:
{text}
"""

# ====== PROMPT 5: tratar secciÃ³n "Responsables del registro" ======
PROMPT5_TEMPLATE = """
Eres un asistente especializado en anonimizar historias clÃ­nicas en espaÃ±ol.

Tu ÃšNICA tarea en este paso es tratar la secciÃ³n "Responsables del registro".

Instrucciones obligatorias:
1) Si en el texto aparece un encabezado "Responsables del registro:" (o una variante muy similar, sin importar mayÃºsculas/minÃºsculas), debes conservar el encabezado tal cual.
2) Para todas las lÃ­neas inmediatamente posteriores a ese encabezado, hasta encontrarte con:
   - una lÃ­nea completamente en blanco, o
   - una nueva secciÃ³n claramente separada (por ejemplo una lÃ­nea que termina en ":" o que estÃ¡ en mayÃºsculas y parece un encabezado),
   reemplaza TODO el contenido de la lÃ­nea por exactamente "[CENSURADO]".
3) No borres lÃ­neas: reemplaza solo su contenido por "[CENSURADO]" pero mantÃ©n la estructura y los saltos de lÃ­nea originales.
4) No agregues comentarios, explicaciones ni encabezados adicionales. Devuelve exclusivamente el texto transformado (o el original si no hay cambios).

Texto a procesar:
{text}
"""

# =======================================
# ---- TOOL CASERA PARA CV_TAG ----
def _parse_number_preserving_sign(num_str: str) -> float:
    s = num_str.strip()
    if not s:
        return 0.0

    sign = -1.0 if s.startswith("-") else 1.0
    if s[0] in "+-":
        s = s[1:].strip()

    if "," in s and "." in s:
        s_clean = s.replace(".", "")
        s_clean = s_clean.replace(",", ".")
    elif "," in s:
        s_clean = s.replace(",", ".")
    elif s.count(".") > 1:
        s_clean = s.replace(".", "")
    else:
        s_clean = s

    s_clean = re.sub(r"[^0-9.]", "", s_clean)
    if not s_clean:
        return 0.0

    try:
        val = float(s_clean)
    except ValueError:
        return 0.0

    return sign * val


def _format_number_like_original(original: str, value: float) -> str:
    s = original.strip()
    if not s:
        return str(int(round(value)))

    had_plus = s.startswith("+")
    if s[0] in "+-":
        s_body = s[1:].strip()
    else:
        s_body = s

    if "," in s_body:
        dec_sep = ","
        parts = s_body.split(",")
        decs = len(parts[1]) if len(parts) > 1 else 0
    elif "." in s_body:
        dec_sep = "."
        parts = s_body.split(".")
        decs = len(parts[1]) if len(parts) > 1 else 0
    else:
        dec_sep = None
        decs = 0

    val = float(value)
    is_neg = val < 0
    val_abs = abs(val)

    if dec_sep is None or decs == 0:
        formatted = str(int(round(val_abs)))
    else:
        formatted = f"{val_abs:.{decs}f}"
        if dec_sep == ",":
            formatted = formatted.replace(".", ",")

    if is_neg:
        formatted = "-" + formatted
    elif had_plus:
        formatted = "+" + formatted

    return formatted


def perturb_cv_tags(text: str) -> str:
    """
    Busca marcas [[CV_TAG: valor]] en el texto y sustituye cada valor
    por una versiÃ³n perturbada Â±50%, manteniendo el signo.
    """
    pattern = re.compile(
        r"\[\[\s*CV_TAG\s*:\s*([-+]?\d[\d\.,]*)\s*\]\]",
        flags=re.IGNORECASE,
    )

    def _repl(match: re.Match) -> str:
        num_str = match.group(1)
        original_val = _parse_number_preserving_sign(num_str)

        if original_val == 0 and re.sub(r"[^0-9]", "", num_str) == "":
            return match.group(0)

        factor = random.uniform(0.5, 1.5)
        new_val = original_val * factor
        new_num_str = _format_number_like_original(num_str, new_val)
        return new_num_str

    return pattern.sub(_repl, text)

# =======================================
# ---- PDF â†’ lista de pÃ¡ginas (PyMuPDF) ----
def pdf_bytes_to_pages(pdf_bytes: bytes) -> List[str]:
    import fitz  # PyMuPDF
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    for page in doc:
        pages.append(page.get_text().strip())
    doc.close()
    return pages

# ---- Llamada a Ollama (streaming) ----
def ollama_generate(
    model: str,
    prompt: str,
    endpoint: str = OLLAMA_ENDPOINT,
    temperature: float = TEMPERATURE,
    options: Optional[dict] = None,
) -> str:
    base_opts = {
        "temperature": temperature,
        "num_ctx": NUM_CTX,
        "num_predict": NUM_PREDICT,
    }
    if options:
        base_opts.update(options)

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True,
        "options": base_opts,
    }
    url = f"{endpoint.rstrip('/')}/api/generate"
    resp = requests.post(url, json=payload, stream=True, timeout=600)
    resp.raise_for_status()

    text_parts = []
    for line in resp.iter_lines():
        if not line:
            continue
        chunk = json.loads(line)
        part = chunk.get("response", "")
        if part:
            text_parts.append(part)
    return "".join(text_parts).strip()

# ---- Chunking por caracteres (solo el TEXTO, no el template) ----
def chunk_text_by_chars(text: str, max_chars: int, overlap: int) -> List[str]:
    if max_chars <= 0:
        return [text]
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(i + max_chars, n)
        chunks.append(text[i:j])
        if j == n:
            break
        i = j - overlap if overlap > 0 else j
    return chunks

# ---- Texto â†’ PDF (PyMuPDF) ----
def text_to_pdf_bytes(
    text: str,
    paper: str = "A4",
    fontname: str = "Courier",
    fontsize: int = 10,
    margin: int = 36,
    line_spacing: float = 1.4,
) -> bytes:
    import fitz
    doc = fitz.open()
    if paper.upper() == "A4":
        width, height = 595, 842
    else:
        width, height = 612, 792

    usable_w = width - 2 * margin
    usable_h = height - 2 * margin

    char_w = fontsize * 0.6
    max_chars_per_line = max(20, int(usable_w / char_w))

    line_h = int(fontsize * line_spacing)
    max_lines_per_page = max(10, int(usable_h / line_h))

    all_lines: List[str] = []
    for para in text.splitlines():
        if not para.strip():
            all_lines.append("")
            continue
        wrapped = textwrap.wrap(para, width=max_chars_per_line, break_long_words=False)
        if not wrapped:
            all_lines.append("")
        else:
            all_lines.extend(wrapped)

    page = None
    x = margin
    y = margin
    lines_on_page = 0

    for line in all_lines:
        if page is None or lines_on_page >= max_lines_per_page:
            page = doc.new_page(width=width, height=height)
            x, y = margin, margin
            lines_on_page = 0
        page.insert_text((x, y), line, fontsize=fontsize, fontname=fontname)
        y += line_h
        lines_on_page += 1

    pdf_bytes = doc.tobytes()
    doc.close()
    return pdf_bytes

# ---- Unir varios PDFs en uno solo (PyMuPDF) ----
def merge_pdfs(pdf_paths: List[str]) -> bytes:
    import fitz
    out_doc = fitz.open()
    for path in pdf_paths:
        src = fitz.open(path)
        out_doc.insert_pdf(src)
        src.close()
    merged_bytes = out_doc.tobytes()
    out_doc.close()
    return merged_bytes

# =======================================
# ---- Helpers Prompt 1 ----
def run_prompt1_on_first_pages(
    pages_text: List[str],
    n_pages: int = 2,
) -> str:
    fragment_text = "\n".join(pages_text[:n_pages]).strip()
    if not fragment_text:
        return ""
    prompt = PROMPT1_TEMPLATE.replace("{text}", fragment_text)
    out = ollama_generate(
        model=MODEL_NAME,
        prompt=prompt,
        endpoint=OLLAMA_ENDPOINT,
        temperature=TEMPERATURE,
    )
    return out.strip()


def parse_llm_list(text: str) -> List[str]:
    t = text.strip()
    start = t.find("[")
    end = t.rfind("]")
    if start != -1 and end != -1:
        t = t[start : end + 1]

    try:
        return json.loads(t)
    except json.JSONDecodeError:
        import ast
        return ast.literal_eval(t)


def postprocess_patient_data(raw_list: List[str]) -> List[str]:
    """
    Ejemplos de entrada:
      ["BENGELO RAKOTO MOHAMUD", "AA 6723.098-0", "ABUBAKAR 431"]

    Salida:
      ["BENGELO", "RAKOTO", "MOHAMUD", "AA67230980", "ABUBAKAR 431"]
    """
    if len(raw_list) < 3:
        raise ValueError(
            f"Se esperaban al menos 3 elementos [nombre, documento, direccion], llegÃ³: {raw_list}"
        )

    full_name = str(raw_list[0]).strip()
    doc       = str(raw_list[1]).strip()
    address   = str(raw_list[2]).strip()

    name_parts = [p for p in full_name.split() if p]

    # dejar solo letras y nÃºmeros en el documento (sin espacios ni puntuaciÃ³n)
    clean_doc = re.sub(r"[^0-9A-Za-z]", "", doc)

    return [*name_parts, clean_doc, address]


def extract_patient_data_chain(pages_text: List[str]) -> Tuple[str, List[str], List[str]]:
    """
    Ejecuta todo Prompt 1:
      - corre el modelo sobre las 2 primeras pÃ¡ginas,
      - parsea la lista,
      - la postprocesa.
    """
    result_prompt_1 = run_prompt1_on_first_pages(pages_text, n_pages=2)
    raw_list = parse_llm_list(result_prompt_1)
    processed_list = postprocess_patient_data(raw_list)
    return result_prompt_1, raw_list, processed_list

# =======================================
# ---- Prompt 2: censurar un bloque usando la lista ----
def censor_block_text(block_text: str, patient_data_list: List[str]) -> str:
    if not block_text.strip():
        return ""

    lista_str = json.dumps(patient_data_list, ensure_ascii=False)

    if USE_CHUNKING and len(block_text) > MAX_CHARS_PER_CHUNK:
        chunks = chunk_text_by_chars(block_text, MAX_CHARS_PER_CHUNK, OVERLAP)
        out_parts: List[str] = []
        for ch in chunks:
            prompt = PROMPT2_TEMPLATE.format(lista=lista_str, text=ch)
            out = ollama_generate(
                model=MODEL_NAME,
                prompt=prompt,
                endpoint=OLLAMA_ENDPOINT,
                temperature=TEMPERATURE,
            )
            out_parts.append(out.strip())
        return "\n\n".join([p for p in out_parts if p]).strip()
    else:
        prompt = PROMPT2_TEMPLATE.format(lista=lista_str, text=block_text)
        out = ollama_generate(
            model=MODEL_NAME,
            prompt=prompt,
            endpoint=OLLAMA_ENDPOINT,
            temperature=TEMPERATURE,
        )
        return out.strip()

# =======================================
# ---- Prompt 3: marcar carga viral en un bloque ----
def tag_cv_in_block_text(block_text: str) -> str:
    if not block_text.strip():
        return ""

    if USE_CHUNKING and len(block_text) > MAX_CHARS_PER_CHUNK:
        chunks = chunk_text_by_chars(block_text, MAX_CHARS_PER_CHUNK, OVERLAP)
        out_parts: List[str] = []
        for ch in chunks:
            prompt = PROMPT3_TEMPLATE.replace("{text}", ch)
            out = ollama_generate(
                model=MODEL_NAME,
                prompt=prompt,
                endpoint=OLLAMA_ENDPOINT,
                temperature=TEMPERATURE,
            )
            out_parts.append(out.strip())
        return "\n\n".join([p for p in out_parts if p]).strip()
    else:
        prompt = PROMPT3_TEMPLATE.replace("{text}", block_text)
        out = ollama_generate(
            model=MODEL_NAME,
            prompt=prompt,
            endpoint=OLLAMA_ENDPOINT,
            temperature=TEMPERATURE,
        )
        return out.strip()

# =======================================
# ---- Prompt 4: anonimizaciÃ³n general en un bloque ----
def general_anon_block_text(block_text: str) -> str:
    if not block_text.strip():
        return ""

    if USE_CHUNKING and len(block_text) > MAX_CHARS_PER_CHUNK:
        chunks = chunk_text_by_chars(block_text, MAX_CHARS_PER_CHUNK, OVERLAP)
        out_parts: List[str] = []
        for ch in chunks:
            prompt = PROMPT4_TEMPLATE.replace("{text}", ch)
            out = ollama_generate(
                model=MODEL_NAME,
                prompt=prompt,
                endpoint=OLLAMA_ENDPOINT,
                temperature=TEMPERATURE,
            )
            out_parts.append(out.strip())
        return "\n\n".join([p for p in out_parts if p]).strip()
    else:
        prompt = PROMPT4_TEMPLATE.replace("{text}", block_text)
        out = ollama_generate(
            model=MODEL_NAME,
            prompt=prompt,
            endpoint=OLLAMA_ENDPOINT,
            temperature=TEMPERATURE,
        )
        return out.strip()

# =======================================
# ---- Prompt 5: tratar secciÃ³n "Responsables del registro" ----
def responsables_block_text(block_text: str) -> str:
    """
    Aplica PROMPT 5 al texto del bloque para censurar la secciÃ³n
    'Responsables del registro' si existe.
    """
    if not block_text.strip():
        return ""

    if USE_CHUNKING and len(block_text) > MAX_CHARS_PER_CHUNK:
        chunks = chunk_text_by_chars(block_text, MAX_CHARS_PER_CHUNK, OVERLAP)
        out_parts: List[str] = []
        for ch in chunks:
            prompt = PROMPT5_TEMPLATE.replace("{text}", ch)
            out = ollama_generate(
                model=MODEL_NAME,
                prompt=prompt,
                endpoint=OLLAMA_ENDPOINT,
                temperature=TEMPERATURE,
            )
            out_parts.append(out.strip())
        return "\n\n".join([p for p in out_parts if p]).strip()
    else:
        prompt = PROMPT5_TEMPLATE.replace("{text}", block_text)
        out = ollama_generate(
            model=MODEL_NAME,
            prompt=prompt,
            endpoint=OLLAMA_ENDPOINT,
            temperature=TEMPERATURE,
        )
        return out.strip()

# =======================================
# ---- Etapa 1 y Etapa 2 por bloque ----
def process_block_stage1(block_text: str, patient_data_list: List[str]) -> str:
    """
    Etapa 1 por bloque:
      - Prompt 2: censurar datos del paciente (lista).
      - Prompt 4: anonimizaciÃ³n general.

    El resultado se pasa a PDF y se guarda en disco para liberar memoria.
    """
    if not block_text.strip():
        return ""

    if DEBUG_PIPELINE:
        print("\n===== Nuevo bloque â€” ETAPA 1 =====")
        print("LEN original bloque:", len(block_text))

    # Prompt 2
    censored_text = censor_block_text(block_text, patient_data_list)
    if DEBUG_PIPELINE:
        print("LEN despuÃ©s de Prompt 2 (censura especÃ­fica):", len(censored_text))

    # Prompt 4
    anon_text = general_anon_block_text(censored_text)
    if DEBUG_PIPELINE:
        print("LEN despuÃ©s de Prompt 4 (anonimizaciÃ³n general):", len(anon_text))

    return anon_text


def process_block_stage2(block_text: str) -> str:
    """
    Etapa 2 por bloque, trabajando sobre el texto ya anonimizado de la Etapa 1:
      - Prompt 5: tratar secciÃ³n 'Responsables del registro'.
      - Prompt 3: marcar carga viral con [[CV_TAG: ...]].
      - Tool casera: perturb_cv_tags para modificar Â±50% esos valores.
    """
    if not block_text.strip():
        return ""

    if DEBUG_PIPELINE:
        print("\n===== ETAPA 2 sobre bloque =====")
        print("LEN entrada Etapa 2:", len(block_text))

    # Prompt 5
    anon_text_resp = responsables_block_text(block_text)
    if DEBUG_PIPELINE:
        print("LEN despuÃ©s de Prompt 5 (responsables):", len(anon_text_resp))

    # Prompt 3
    tagged_text = tag_cv_in_block_text(anon_text_resp)
    if DEBUG_PIPELINE:
        print("LEN despuÃ©s de Prompt 3 (tag CV_TAG):", len(tagged_text))

    # Tool casera
    final_text = perturb_cv_tags(tagged_text)
    if DEBUG_PIPELINE:
        print("LEN final despuÃ©s de perturb_cv_tags:", len(final_text))

    return final_text

# =======================================
# ---- CORE: PDF â†’ PDF final usando 2 etapas por bloque ----
def full_pipeline_pdf_pages_to_merged_pdf(
    pages_text: List[str],
    patient_data_list: List[str],
    pages_per_block: int = PAGES_PER_BLOCK,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    tempdir_callback: Optional[Callable[[str], None]] = None,
) -> bytes:
    """
    Para cada bloque de pÃ¡ginas:
      1) Une las pÃ¡ginas en texto.
      2) Etapa 1 (Prompt 2 + Prompt 4) â†’ texto1.
      3) Texto1 â†’ PDF intermedio en carpeta temporal.
      4) Lee ese PDF intermedio, lo pasa de nuevo a texto.
      5) Etapa 2 (Prompt 5 + Prompt 3 + tool casera) â†’ texto_final.
      6) texto_final â†’ PDF final de bloque.
    Al final, se unen todos los PDFs finales.
    """
    num_pages = len(pages_text)
    if num_pages == 0:
        return b""

    temp_dir = tempfile.mkdtemp(prefix="anon_blocks_")

    if tempdir_callback is not None:
        tempdir_callback(temp_dir)

    final_block_paths: List[str] = []

    try:
        blocks = []
        for start in range(0, num_pages, pages_per_block):
            end = min(start + pages_per_block, num_pages)
            blocks.append((start, end))

        total_blocks = len(blocks)

        for block_idx, (start_idx, end_idx) in enumerate(blocks, start=1):
            # ============================
            # Tomar pÃ¡ginas del bloque
            # ============================
            block_pages = pages_text[start_idx:end_idx]
            block_text = "\n".join(block_pages).strip()

            # ============================
            # ETAPA 1: Prompt 2 + Prompt 4
            # ============================
            text_stage1 = process_block_stage1(block_text, patient_data_list)
            pdf_stage1_bytes = text_to_pdf_bytes(text_stage1)

            stage1_path = os.path.join(temp_dir, f"block_{block_idx:04d}_stage1.pdf")
            with open(stage1_path, "wb") as f:
                f.write(pdf_stage1_bytes)

            # Liberar memoria de Etapa 1
            del block_pages
            del block_text
            del text_stage1
            del pdf_stage1_bytes

            # ============================
            # ETAPA 2: leer PDF intermedio y terminar pipeline
            # ============================
            with open(stage1_path, "rb") as f:
                stage1_pdf_bytes = f.read()

            # Pasar PDF intermedio a texto
            stage1_pages_text = pdf_bytes_to_pages(stage1_pdf_bytes)
            stage1_full_text = "\n".join(stage1_pages_text).strip()

            # Etapa 2: Prompt 5 + Prompt 3 + tool casera
            text_final = process_block_stage2(stage1_full_text)
            pdf_final_bytes = text_to_pdf_bytes(text_final)

            final_path = os.path.join(temp_dir, f"block_{block_idx:04d}_final.pdf")
            with open(final_path, "wb") as f:
                f.write(pdf_final_bytes)

            final_block_paths.append(final_path)

            # Liberar memoria de Etapa 2
            del stage1_pdf_bytes
            del stage1_pages_text
            del stage1_full_text
            del text_final
            del pdf_final_bytes

            # Borrar PDF intermedio de Etapa 1 del disco
            try:
                os.remove(stage1_path)
            except Exception:
                pass

            # Progreso
            if progress_callback is not None:
                progress_callback(block_idx, total_blocks)

        # Unir todos los PDFs finales
        merged_pdf_bytes = merge_pdfs(sorted(final_block_paths))
        return merged_pdf_bytes

    finally:
        try:
            shutil.rmtree(temp_dir)
        except Exception:
            pass

# =======================================
# ---- UI Streamlit ----
def main():
    st.set_page_config(page_title="PDF â†’ ðŸ§  Qwen 8B (Ollama)", layout="centered")
    st.title("ðŸ“„ PDF â†’ ðŸ§  Qwen 8B (Ollama)")

    uploaded = st.file_uploader("SubÃ­ un PDF", type=["pdf"])

    if uploaded is not None:
        pdf_bytes = uploaded.read()
        with st.spinner("Extrayendo texto del PDF..."):
            try:
                pages_text = pdf_bytes_to_pages(pdf_bytes)
            except Exception as e:
                st.error(f"Error al leer el PDF: {e}")
                st.stop()

        num_pages = len(pages_text)
        full_text = "\n".join(pages_text).strip()

        st.success(f"PDF leÃ­do correctamente. PÃ¡ginas detectadas: {num_pages}")
        st.caption(
            f"Caracteres extraÃ­dos (total): {len(full_text)} | "
            f"chunk: {MAX_CHARS_PER_CHUNK} | overlap: {OVERLAP} | "
            f"bloque de pÃ¡ginas: {PAGES_PER_BLOCK}"
        )
        st.text_area(
            "Vista previa del texto (primeras pÃ¡ginas)",
            value=full_text[:2000] + ("..." if len(full_text) > 2000 else ""),
            height=200,
        )

        if st.button("ðŸš€ Ejecutar modelo (P1 + (P2+P4) + (P5+P3+CV_TOOL))"):
            # Paso 1: Prompt 1
            with st.spinner("Paso 1: ejecutando Prompt 1 (extraer datos del encabezado)..."):
                try:
                    result_prompt_1, raw_list, patient_data_list = extract_patient_data_chain(
                        pages_text
                    )
                except Exception as e:
                    st.error(f"Error en Prompt 1: {e}")
                    st.stop()

            st.session_state["prompt1_raw_response"] = result_prompt_1
            st.session_state["prompt1_raw_list"] = raw_list
            st.session_state["patient_data_list"] = patient_data_list

            st.success("Prompt 1 completado. Datos detectados:")
            st.write("Lista procesada (nombres, doc, direcciÃ³n):", patient_data_list)

            # Paso 2+: pipeline por bloques en 2 etapas
            progress_bar = st.progress(0)
            status_placeholder = st.empty()
            tempdir_placeholder = st.empty()

            def progress_cb(current_block: int, total_blocks: int) -> None:
                pct = int(current_block / total_blocks * 100)
                progress_bar.progress(pct)
                status_placeholder.write(
                    f"Procesando bloque {current_block} de {total_blocks} (2 etapas por bloque)..."
                )

            def tempdir_cb(path: str) -> None:
                tempdir_placeholder.caption(f"Carpeta temporal usada: `{path}`")

            with st.spinner(
                "Procesando bloques en 2 etapas: (P2+P4) y luego (P5+P3+CV_TOOL)..."
            ):
                try:
                    final_pdf_bytes = full_pipeline_pdf_pages_to_merged_pdf(
                        pages_text,
                        patient_data_list=patient_data_list,
                        pages_per_block=PAGES_PER_BLOCK,
                        progress_callback=progress_cb,
                        tempdir_callback=tempdir_cb,
                    )
                except Exception as e:
                    st.error(f"Error durante el procesamiento por bloques: {e}")
                    st.stop()

            progress_bar.progress(100)
            status_placeholder.write("Procesamiento completado âœ…")

            if not final_pdf_bytes:
                st.warning(
                    "La salida estÃ¡ vacÃ­a. RevisÃ¡ el PDF original o ajustÃ¡ parÃ¡metros."
                )
            else:
                st.subheader("ðŸ“„ Descarga")
                st.download_button(
                    "ðŸ“¥ Descargar PDF anonimizado",
                    data=io.BytesIO(final_pdf_bytes),
                    file_name="salida_ollama_anonimizada.pdf",
                    mime="application/pdf",
                )
                st.success("Â¡Listo! Se ejecutÃ³ el pipeline completo en 2 etapas por bloque y se generÃ³ el PDF anonimizado âœ…")

    else:
        st.info("SubÃ­ un PDF para comenzar.")


if __name__ == "__main__":
    main()
